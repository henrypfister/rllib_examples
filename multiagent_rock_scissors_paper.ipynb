{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nhf3JVxG1E0z"
   },
   "outputs": [],
   "source": [
    "# Comment these lines if not using Google Colaboratory\n",
    "!pip install -U ray[rllib]\n",
    "!pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pj0n3d7LX81u"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define Rock Paper Scissors (taken from Ray example)\n",
    "#\n",
    "\n",
    "import ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Tuple, Discrete\n",
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "\n",
    "class RockPaperScissorsEnv(MultiAgentEnv):\n",
    "    \"\"\"Two-player environment for rock paper scissors.\n",
    "    The observation is simply the last opponent action.\"\"\"\n",
    "\n",
    "    def __init__(self, _):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Discrete(3)\n",
    "        self.player1 = \"agent_1\"\n",
    "        self.player2 = \"agent_2\"\n",
    "        self.last_move = None\n",
    "        self.num_moves = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_move = (1, 2)\n",
    "        self.num_moves = 0\n",
    "        return {\n",
    "            self.player1: self.last_move[1],\n",
    "            self.player2: self.last_move[0],\n",
    "        }\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        move1 = action_dict[self.player1]\n",
    "        move2 = action_dict[self.player2]\n",
    "        self.last_move = (move1, move2)\n",
    "        obs = {\n",
    "            self.player1: self.last_move[1],\n",
    "            self.player2: self.last_move[0],\n",
    "        }\n",
    "        r1, r2 = {\n",
    "            (ROCK, ROCK): (0, 0),\n",
    "            (ROCK, PAPER): (-1, 1),\n",
    "            (ROCK, SCISSORS): (1, -1),\n",
    "            (PAPER, ROCK): (1, -1),\n",
    "            (PAPER, PAPER): (0, 0),\n",
    "            (PAPER, SCISSORS): (-1, 1),\n",
    "            (SCISSORS, ROCK): (-1, 1),\n",
    "            (SCISSORS, PAPER): (1, -1),\n",
    "            (SCISSORS, SCISSORS): (0, 0),\n",
    "        }[move1, move2]\n",
    "        rew = {\n",
    "            self.player1: r1,\n",
    "            self.player2: r2,\n",
    "        }\n",
    "        self.num_moves += 1\n",
    "        done = {\n",
    "            \"__all__\": self.num_moves >= 10,\n",
    "        }\n",
    "        return obs, rew, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        print(self.last_move)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_cxE0BIXo1A"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Test with IQL\n",
    "#\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "import ray.rllib.agents.dqn as dqn\n",
    "import ray.rllib.agents.dqn.apex as apex\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.agents.dqn.dqn import DQNTrainer as DQNAgent\n",
    "from ray.rllib.agents.dqn.dqn_policy import DQNTFPolicy as DQNPolicyGraph\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Tuple, Discrete\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(webui_host=\"127.0.0.1\",ignore_reinit_error=True, log_to_driver=False)\n",
    "\n",
    "game = RockPaperScissorsEnv({})\n",
    "act_space = game.action_space\n",
    "obs_space = game.observation_space\n",
    "\n",
    "trainer = DQNAgent(env=RockPaperScissorsEnv, config={\n",
    "    \"gamma\": 0.95,\n",
    "    \"lr\": 0.0001,\n",
    "    \"schedule_max_timesteps\": 20000,\n",
    "    \"timesteps_per_iteration\": 1000,\n",
    "    \"exploration_fraction\": 0.95,\n",
    "    \"exploration_final_eps\": 0.02,\n",
    "    \"model\": {\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "        \"fcnet_hiddens\": [32,16],\n",
    "    },\n",
    "    \"multiagent\": {\n",
    "        \"policies\": {\n",
    "            \"dqn_policy1\": (DQNPolicyGraph, obs_space, act_space, {\"gamma\": 0.95}),\n",
    "            \"dqn_policy2\": (DQNPolicyGraph, obs_space, act_space, {\"gamma\": 0.95}),\n",
    "        },\n",
    "        \"policies_to_train\": [\"dqn_policy1\",\"dqn_policy2\"],\n",
    "        \"policy_mapping_fn\":\n",
    "            lambda agent_id:\n",
    "                 \"dqn_policy1\"\n",
    "                 if int(agent_id.split('_')[1])<=1\n",
    "                 else \"dqn_policy2\"\n",
    "    },\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gq5AAE8ZYyre"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "N = 20\n",
    "mean_reward = np.zeros(N)\n",
    "mean_length = np.zeros(N)\n",
    "for i in range(N):\n",
    "    # Improve the DQN policy\n",
    "    stats = trainer.train()\n",
    "    print(\"== Iteration\", i, \"== AvgReward:\",stats[\"policy_reward_mean\"],\"== AvgLength:\",stats[\"episode_len_mean\"])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzpJ1oqV7q7q"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "agent_names = [\"agent_%d\"%(i+1) for i in range(2)]\n",
    "policy_map = trainer.config[\"multiagent\"][\"policy_mapping_fn\"]\n",
    "\n",
    "N = 100\n",
    "for i in range(N):\n",
    "    #print(\"Game \",i)\n",
    "    obs = game.reset()\n",
    "    #game.render()\n",
    "    dones = {}\n",
    "    \n",
    "    avgR = np.zeros(2)\n",
    "    R = np.zeros(2)\n",
    "    t = 0\n",
    "    action_dict = {}\n",
    "    while not dones.get(\"__all__\",False):\n",
    "        # Get actions from neural network\n",
    "        action_dict = { x: trainer.compute_action(obs[x],policy_id=policy_map(x)) for x in agent_names}\n",
    "        \n",
    "        # Make move\n",
    "        obs, rewards, dones, _ = game.step(action_dict)\n",
    "        \n",
    "        # Add rewards up\n",
    "        for q in rewards:\n",
    "            index = int(q.split('_')[1])-1\n",
    "            R[index] += rewards[q]\n",
    "\n",
    "        #game.render()\n",
    "        \n",
    "    avgR = avgR + R\n",
    "       \n",
    "print('average reward:',avgR/N)  \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rllib_rsp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
