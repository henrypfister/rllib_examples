{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rllib_blackjack",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhf3JVxG1E0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comment these lines if not using Google Colaboratory\n",
        "!pip install -U ray[rllib]\n",
        "!pip install gputil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3iTx8781mN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "import ray\n",
        "import ray.rllib.agents.ppo as ppo\n",
        "import ray.rllib.agents.dqn as dqn\n",
        "import ray.rllib.agents.dqn.apex as apex\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(webui_host=\"127.0.0.1\",ignore_reinit_error=True, log_to_driver=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfg_eRDj_f1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use apex?\n",
        "ape = 0\n",
        "if ape:\n",
        "    config = apex.APEX_DEFAULT_CONFIG.copy()\n",
        "    config[\"min_iter_time_s\"] = 2\n",
        "    config[\"lr\"] = 0.000005\n",
        "    config[\"buffer_size\"] = 200000\n",
        "    config[\"timesteps_per_iteration\"] = 8000\n",
        "    config[\"n_step\"] = 4\n",
        "    niter = 30\n",
        "else:\n",
        "    config = dqn.DEFAULT_CONFIG.copy()\n",
        "    config[\"schedule_max_timesteps\"] = 60000\n",
        "    config[\"timesteps_per_iteration\"]: 4000\n",
        "    niter = 18\n",
        "\n",
        "# Config\n",
        "#config[\"num_envs_per_worker\"] = 8\n",
        "config[\"num_gpus\"] = 1\n",
        "config[\"num_workers\"] = 2\n",
        "config[\"gamma\"] = 0.95\n",
        "\n",
        "if ape:\n",
        "    agent = apex.ApexTrainer(config=config, env=\"Blackjack-v0\")\n",
        "else:\n",
        "    agent = dqn.DQNTrainer(config=config, env=\"Blackjack-v0\")\n",
        "\n",
        "for i in range(niter):\n",
        "    # Perform one iteration of training the policy\n",
        "    result = agent.train()\n",
        "    print(\"Iteration\", i, \"-- Mean Reward: \",result[\"episode_reward_mean\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esvDiFT8BJ7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate\n",
        "env = gym.make(\"Blackjack-v0\")\n",
        "\n",
        "avgR = 0\n",
        "N=5000\n",
        "for i in range(N):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    R = 0\n",
        "    while not done:\n",
        "        action = agent.compute_action(obs)\n",
        "        obs, r, done, _ = env.step(action)\n",
        "        R += r\n",
        "\n",
        "    avgR = avgR + R\n",
        "\n",
        "print('Evaluation mean reward:',avgR/N)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}